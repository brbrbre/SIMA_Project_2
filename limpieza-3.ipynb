{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b0b1625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INICIANDO PLAN DE CONCATENACIÓN DE DATOS NOROESTE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# PLAN DE TRABAJO: CONCATENACIÓN DE DATOS NOROESTE\n",
    "# =================================================\n",
    "# \n",
    "# OBJETIVO: Concatenar datos de NOROESTE, NOROESTE2 y NOROESTE3 de todos los archivos\n",
    "# \n",
    "# ESTRUCTURAS IDENTIFICADAS:\n",
    "# 1. 2020-2021, 2022-2023: Hojas separadas (NOROESTE, NOROESTE2, NOROESTE3)\n",
    "# 2. 2023-2024: Una hoja con columnas agrupadas (NOROESTE, NOROESTE 2, NOROESTE 3)\n",
    "# 3. 2024: Hojas separadas con nombres diferentes (NE, NE2, NE3)\n",
    "#\n",
    "# ESTRATEGIA:\n",
    "# - Función específica para cada tipo de estructura\n",
    "# - Estandarización de nombres de columnas\n",
    "# - Concatenación temporal de todas las estaciones\n",
    "# - Control de calidad de datos\n",
    "\n",
    "print(\"INICIANDO PLAN DE CONCATENACIÓN DE DATOS NOROESTE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "678ce45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo datos 2020-2021 (hojas separadas)...\n",
      "  [OK] NOROESTE: 17537 registros\n",
      "  [OK] NOROESTE: 17537 registros\n",
      "  [OK] NOROESTE2: 17535 registros\n",
      "  [OK] NOROESTE3: 0 registros\n",
      "  [OK] NOROESTE2: 17535 registros\n",
      "  [OK] NOROESTE3: 0 registros\n"
     ]
    }
   ],
   "source": [
    "# FUNCIÓN 1: Leer datos de archivos con hojas separadas (2020-2021, 2022-2023)\n",
    "def read_noroeste_separate_sheets(file_path, year_range):\n",
    "    \"\"\"\n",
    "    Lee datos de NOROESTE, NOROESTE2 y NOROESTE3 de archivos con hojas separadas\n",
    "    \"\"\"\n",
    "    print(f\"Leyendo datos {year_range} (hojas separadas)...\")\n",
    "    \n",
    "    noroeste_data = {}\n",
    "    stations = ['NOROESTE', 'NOROESTE2', 'NOROESTE3']\n",
    "    \n",
    "    try:\n",
    "        for station in stations:\n",
    "            df = pd.read_excel(file_path, sheet_name=station)\n",
    "            \n",
    "            # Estandarizar nombre de columna de fecha\n",
    "            if 'date' in df.columns:\n",
    "                df = df.rename(columns={'date': 'datetime'})\n",
    "            \n",
    "            # Añadir información de estación y período\n",
    "            df['station'] = station\n",
    "            df['year_range'] = year_range\n",
    "            df['source_file'] = os.path.basename(file_path)\n",
    "            \n",
    "            noroeste_data[station] = df\n",
    "            print(f\"  [OK] {station}: {len(df)} registros\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  [ERROR] Error leyendo {year_range}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return noroeste_data\n",
    "\n",
    "# Test con archivo 2020-2021\n",
    "file_2020_2021 = 'Bases_Datos/DATOS HISTÓRICOS 2020_2021_TODAS ESTACIONES.xlsx'\n",
    "data_2020_2021 = read_noroeste_separate_sheets(file_2020_2021, '2020-2021')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fd42fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo datos 2023-2024 (columnas agrupadas)...\n",
      "  [OK] NOROESTE: 13870 registros, 15 columnas\n",
      "  [OK] NOROESTE2: 13870 registros, 15 columnas\n",
      "  [OK] NOROESTE3: 13870 registros, 15 columnas\n",
      "  [OK] NOROESTE: 13870 registros, 15 columnas\n",
      "  [OK] NOROESTE2: 13870 registros, 15 columnas\n",
      "  [OK] NOROESTE3: 13870 registros, 15 columnas\n"
     ]
    }
   ],
   "source": [
    "# FUNCIÓN 2: Leer datos de archivo con columnas agrupadas (2023-2024)\n",
    "def read_noroeste_grouped_columns(file_path, year_range):\n",
    "    \"\"\"\n",
    "    Lee datos de NOROESTE del archivo 2023-2024 donde están en columnas agrupadas\n",
    "    \"\"\"\n",
    "    print(f\"Leyendo datos {year_range} (columnas agrupadas)...\")\n",
    "    \n",
    "    try:\n",
    "        # Leer el archivo completo\n",
    "        df_full = pd.read_excel(file_path, sheet_name='Param_horarios_Estaciones')\n",
    "        \n",
    "        noroeste_data = {}\n",
    "        \n",
    "        # Identificar grupos de columnas para cada estación NOROESTE\n",
    "        station_groups = {\n",
    "            'NOROESTE': [col for col in df_full.columns if col.startswith('NOROESTE') and not col.startswith('NOROESTE ')],\n",
    "            'NOROESTE2': [col for col in df_full.columns if col.startswith('NOROESTE 2')],\n",
    "            'NOROESTE3': [col for col in df_full.columns if col.startswith('NOROESTE 3')]\n",
    "        }\n",
    "        \n",
    "        for station, cols in station_groups.items():\n",
    "            if cols:\n",
    "                # Extraer datos de las columnas correspondientes\n",
    "                station_df = df_full[['Unnamed: 0'] + cols].copy()\n",
    "                \n",
    "                # La primera fila contiene los nombres de parámetros\n",
    "                # La segunda fila contiene las unidades\n",
    "                # Los datos empiezan desde la tercera fila\n",
    "                param_names = station_df.iloc[0, 1:].values  # Parámetros\n",
    "                units = station_df.iloc[1, 1:].values        # Unidades\n",
    "                \n",
    "                # Crear nuevos nombres de columnas combinando parámetro y unidad\n",
    "                new_columns = ['datetime']\n",
    "                for param, unit in zip(param_names, units):\n",
    "                    if pd.notna(param) and pd.notna(unit):\n",
    "                        new_columns.append(f\"{param}\")\n",
    "                    elif pd.notna(param):\n",
    "                        new_columns.append(f\"{param}\")\n",
    "                    else:\n",
    "                        new_columns.append(\"unknown\")\n",
    "                \n",
    "                # Crear DataFrame con los datos (desde fila 2 en adelante)\n",
    "                data_rows = station_df.iloc[2:].reset_index(drop=True)\n",
    "                data_rows.columns = new_columns[:len(data_rows.columns)]\n",
    "                \n",
    "                # Añadir información de estación y período\n",
    "                data_rows['station'] = station\n",
    "                data_rows['year_range'] = year_range\n",
    "                data_rows['source_file'] = os.path.basename(file_path)\n",
    "                \n",
    "                noroeste_data[station] = data_rows\n",
    "                print(f\"  [OK] {station}: {len(data_rows)} registros, {len(cols)} columnas\")\n",
    "            else:\n",
    "                print(f\"  [WARNING] {station}: No se encontraron columnas\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"  [ERROR] Error leyendo {year_range}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return noroeste_data\n",
    "\n",
    "# Test con archivo 2023-2024\n",
    "file_2023_2024 = 'Bases_Datos/DATOS HISTÓRICOS 2023_2024_TODAS ESTACIONES_ITESM.xlsx'\n",
    "data_2023_2024 = read_noroeste_grouped_columns(file_2023_2024, '2023-2024')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "801761bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo datos 2024 (hojas renombradas)...\n",
      "  [OK] NOROESTE (hoja NE): 8784 registros\n",
      "  [OK] NOROESTE (hoja NE): 8784 registros\n",
      "  [OK] NOROESTE2 (hoja NE2): 8782 registros\n",
      "  [OK] NOROESTE2 (hoja NE2): 8782 registros\n",
      "  [OK] NOROESTE3 (hoja NE3): 8782 registros\n",
      "  [OK] NOROESTE3 (hoja NE3): 8782 registros\n"
     ]
    }
   ],
   "source": [
    "# FUNCIÓN 3: Leer datos de archivo 2024 (hojas con nombres diferentes)\n",
    "def read_noroeste_renamed_sheets(file_path, year_range):\n",
    "    \"\"\"\n",
    "    Lee datos de NOROESTE del archivo 2024 donde las hojas tienen nombres NE, NE2, NE3\n",
    "    \"\"\"\n",
    "    print(f\"Leyendo datos {year_range} (hojas renombradas)...\")\n",
    "    \n",
    "    noroeste_data = {}\n",
    "    # Mapeo de nombres de hojas a nombres de estaciones\n",
    "    sheet_mapping = {\n",
    "        'NE': 'NOROESTE',\n",
    "        'NE2': 'NOROESTE2', \n",
    "        'NE3': 'NOROESTE3'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        for sheet_name, station_name in sheet_mapping.items():\n",
    "            df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "            \n",
    "            # Estandarizar nombre de columna de fecha\n",
    "            if 'Fecha y hora' in df.columns:\n",
    "                df = df.rename(columns={'Fecha y hora': 'datetime'})\n",
    "            \n",
    "            # Limpiar nombres de columnas (remover unidades entre paréntesis para estandarizar)\n",
    "            df.columns = [col.split(' (')[0] if '(' in col else col for col in df.columns]\n",
    "            \n",
    "            # Añadir información de estación y período\n",
    "            df['station'] = station_name\n",
    "            df['year_range'] = year_range\n",
    "            df['source_file'] = os.path.basename(file_path)\n",
    "            \n",
    "            noroeste_data[station_name] = df\n",
    "            print(f\"  [OK] {station_name} (hoja {sheet_name}): {len(df)} registros\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  [ERROR] Error leyendo {year_range}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    return noroeste_data\n",
    "\n",
    "# Test con archivo 2024\n",
    "file_2024 = 'Bases_Datos/DATOS HISTÓRICOS 2024_TODAS ESTACIONES.xlsx'\n",
    "data_2024 = read_noroeste_renamed_sheets(file_2024, '2024')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12038679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCIÓN 4: Estandarizar columnas y concatenar datos\n",
    "def standardize_and_concatenate(all_data_dict):\n",
    "    \"\"\"\n",
    "    Estandariza nombres de columnas y concatena todos los datos en un DataFrame único\n",
    "    \"\"\"\n",
    "    print(\"Estandarizando y concatenando datos...\")\n",
    "    \n",
    "    all_dataframes = []\n",
    "    \n",
    "    # Mapeo de nombres de columnas comunes para estandarización\n",
    "    column_mapping = {\n",
    "        'CO': 'CO', 'co': 'CO',\n",
    "        'NO': 'NO', 'no': 'NO', \n",
    "        'NO2': 'NO2', 'no2': 'NO2',\n",
    "        'NOX': 'NOX', 'nox': 'NOX',\n",
    "        'O3': 'O3', 'o3': 'O3',\n",
    "        'PM10': 'PM10', 'pm10': 'PM10',\n",
    "        'PM2.5': 'PM25', 'pm2.5': 'PM25', 'PM2': 'PM25',\n",
    "        'PRS': 'PRS', 'prs': 'PRS',\n",
    "        'RAINF': 'RAINF', 'rainf': 'RAINF',\n",
    "        'RH': 'RH', 'rh': 'RH',\n",
    "        'TEMP': 'TEMP', 'temp': 'TEMP',\n",
    "        'WSR': 'WSR', 'wsr': 'WSR',\n",
    "        'WDR': 'WDR', 'wdr': 'WDR'\n",
    "    }\n",
    "    \n",
    "    for year_range, year_data in all_data_dict.items():\n",
    "        if year_data is None:\n",
    "            continue\n",
    "            \n",
    "        for station, df in year_data.items():\n",
    "            if df is not None and not df.empty:\n",
    "                # Crear copia para no modificar original\n",
    "                df_clean = df.copy()\n",
    "                \n",
    "                # Estandarizar nombres de columnas\n",
    "                df_clean.columns = [column_mapping.get(col, col) for col in df_clean.columns]\n",
    "                \n",
    "                # Asegurar que datetime sea datetime\n",
    "                if 'datetime' in df_clean.columns:\n",
    "                    df_clean['datetime'] = pd.to_datetime(df_clean['datetime'], errors='coerce')\n",
    "                \n",
    "                # Añadir a la lista\n",
    "                all_dataframes.append(df_clean)\n",
    "                print(f\"  [OK] Añadido: {station} ({year_range}) - {len(df_clean)} registros\")\n",
    "    \n",
    "    if all_dataframes:\n",
    "        # Concatenar todos los DataFrames\n",
    "        final_df = pd.concat(all_dataframes, ignore_index=True, sort=False)\n",
    "        \n",
    "        # Ordenar por datetime\n",
    "        final_df = final_df.sort_values('datetime').reset_index(drop=True)\n",
    "        \n",
    "        print(f\"\\nCONCATENACIÓN COMPLETADA:\")\n",
    "        print(f\"   Total de registros: {len(final_df)}\")\n",
    "        print(f\"   Rango de fechas: {final_df['datetime'].min()} a {final_df['datetime'].max()}\")\n",
    "        print(f\"   Estaciones incluidas: {final_df['station'].unique()}\")\n",
    "        print(f\"   Columnas disponibles: {len(final_df.columns)}\")\n",
    "        \n",
    "        return final_df\n",
    "    else:\n",
    "        print(\"[ERROR] No se encontraron datos para concatenar\")\n",
    "        return None\n",
    "\n",
    "# FUNCIÓN 5: Función principal para ejecutar todo el proceso\n",
    "def process_all_noroeste_data():\n",
    "    \"\"\"\n",
    "    Función principal que ejecuta todo el proceso de concatenación\n",
    "    \"\"\"\n",
    "    print(\"INICIANDO PROCESO COMPLETO DE CONCATENACIÓN\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    all_data = {}\n",
    "    \n",
    "    # 1. Leer datos 2020-2021\n",
    "    file_2020_2021 = 'Bases_Datos/DATOS HISTÓRICOS 2020_2021_TODAS ESTACIONES.xlsx'\n",
    "    if os.path.exists(file_2020_2021):\n",
    "        all_data['2020-2021'] = read_noroeste_separate_sheets(file_2020_2021, '2020-2021')\n",
    "    \n",
    "    # 2. Leer datos 2022-2023\n",
    "    file_2022_2023 = 'Bases_Datos/DATOS HISTÓRICOS 2022_2023_TODAS ESTACIONES.xlsx'\n",
    "    if os.path.exists(file_2022_2023):\n",
    "        all_data['2022-2023'] = read_noroeste_separate_sheets(file_2022_2023, '2022-2023')\n",
    "    \n",
    "    # 3. Leer datos 2023-2024\n",
    "    file_2023_2024 = 'Bases_Datos/DATOS HISTÓRICOS 2023_2024_TODAS ESTACIONES_ITESM.xlsx'\n",
    "    if os.path.exists(file_2023_2024):\n",
    "        all_data['2023-2024'] = read_noroeste_grouped_columns(file_2023_2024, '2023-2024')\n",
    "    \n",
    "    # 4. Leer datos 2024\n",
    "    file_2024 = 'Bases_Datos/DATOS HISTÓRICOS 2024_TODAS ESTACIONES.xlsx'\n",
    "    if os.path.exists(file_2024):\n",
    "        all_data['2024'] = read_noroeste_renamed_sheets(file_2024, '2024')\n",
    "    \n",
    "    # 5. Estandarizar y concatenar\n",
    "    final_dataset = standardize_and_concatenate(all_data)\n",
    "    \n",
    "    return final_dataset, all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0544507e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EJECUTANDO PROCESO DE CONCATENACIÓN COMPLETO\n",
      "============================================================\n",
      "INICIANDO PROCESO COMPLETO DE CONCATENACIÓN\n",
      "============================================================\n",
      "Leyendo datos 2020-2021 (hojas separadas)...\n",
      "  [OK] NOROESTE: 17537 registros\n",
      "  [OK] NOROESTE: 17537 registros\n",
      "  [OK] NOROESTE2: 17535 registros\n",
      "  [OK] NOROESTE3: 0 registros\n",
      "Leyendo datos 2022-2023 (hojas separadas)...\n",
      "  [OK] NOROESTE2: 17535 registros\n",
      "  [OK] NOROESTE3: 0 registros\n",
      "Leyendo datos 2022-2023 (hojas separadas)...\n",
      "  [OK] NOROESTE: 14255 registros\n",
      "  [OK] NOROESTE: 14255 registros\n",
      "  [OK] NOROESTE2: 14255 registros\n",
      "  [OK] NOROESTE2: 14255 registros\n",
      "  [OK] NOROESTE3: 6237 registros\n",
      "Leyendo datos 2023-2024 (columnas agrupadas)...\n",
      "  [OK] NOROESTE3: 6237 registros\n",
      "Leyendo datos 2023-2024 (columnas agrupadas)...\n",
      "  [OK] NOROESTE: 13870 registros, 15 columnas\n",
      "  [OK] NOROESTE2: 13870 registros, 15 columnas\n",
      "  [OK] NOROESTE3: 13870 registros, 15 columnas\n",
      "Leyendo datos 2024 (hojas renombradas)...\n",
      "  [OK] NOROESTE: 13870 registros, 15 columnas\n",
      "  [OK] NOROESTE2: 13870 registros, 15 columnas\n",
      "  [OK] NOROESTE3: 13870 registros, 15 columnas\n",
      "Leyendo datos 2024 (hojas renombradas)...\n",
      "  [OK] NOROESTE (hoja NE): 8784 registros\n",
      "  [OK] NOROESTE (hoja NE): 8784 registros\n",
      "  [OK] NOROESTE2 (hoja NE2): 8782 registros\n",
      "  [OK] NOROESTE2 (hoja NE2): 8782 registros\n",
      "  [OK] NOROESTE3 (hoja NE3): 8782 registros\n",
      "Estandarizando y concatenando datos...\n",
      "  [OK] Añadido: NOROESTE (2020-2021) - 17537 registros\n",
      "  [OK] Añadido: NOROESTE2 (2020-2021) - 17535 registros\n",
      "  [OK] Añadido: NOROESTE (2022-2023) - 14255 registros\n",
      "  [OK] Añadido: NOROESTE2 (2022-2023) - 14255 registros\n",
      "  [OK] Añadido: NOROESTE3 (2022-2023) - 6237 registros\n",
      "  [OK] Añadido: NOROESTE (2023-2024) - 13870 registros\n",
      "  [OK] Añadido: NOROESTE2 (2023-2024) - 13870 registros\n",
      "  [OK] Añadido: NOROESTE3 (2023-2024) - 13870 registros\n",
      "  [OK] Añadido: NOROESTE (2024) - 8784 registros\n",
      "  [OK] Añadido: NOROESTE2 (2024) - 8782 registros\n",
      "  [OK] Añadido: NOROESTE3 (2024) - 8782 registros\n",
      "\n",
      "CONCATENACIÓN COMPLETADA:\n",
      "   Total de registros: 137777\n",
      "   Rango de fechas: 2020-01-01 00:00:00 a 2024-12-31 23:00:00\n",
      "   Estaciones incluidas: ['NOROESTE' 'NOROESTE2' 'NOROESTE3']\n",
      "   Columnas disponibles: 20\n",
      "\n",
      "RESUMEN DETALLADO:\n",
      "----------------------------------------\n",
      "Registros por estación:\n",
      "  NOROESTE:\n",
      "     Registros: 54446\n",
      "  [OK] NOROESTE3 (hoja NE3): 8782 registros\n",
      "Estandarizando y concatenando datos...\n",
      "  [OK] Añadido: NOROESTE (2020-2021) - 17537 registros\n",
      "  [OK] Añadido: NOROESTE2 (2020-2021) - 17535 registros\n",
      "  [OK] Añadido: NOROESTE (2022-2023) - 14255 registros\n",
      "  [OK] Añadido: NOROESTE2 (2022-2023) - 14255 registros\n",
      "  [OK] Añadido: NOROESTE3 (2022-2023) - 6237 registros\n",
      "  [OK] Añadido: NOROESTE (2023-2024) - 13870 registros\n",
      "  [OK] Añadido: NOROESTE2 (2023-2024) - 13870 registros\n",
      "  [OK] Añadido: NOROESTE3 (2023-2024) - 13870 registros\n",
      "  [OK] Añadido: NOROESTE (2024) - 8784 registros\n",
      "  [OK] Añadido: NOROESTE2 (2024) - 8782 registros\n",
      "  [OK] Añadido: NOROESTE3 (2024) - 8782 registros\n",
      "\n",
      "CONCATENACIÓN COMPLETADA:\n",
      "   Total de registros: 137777\n",
      "   Rango de fechas: 2020-01-01 00:00:00 a 2024-12-31 23:00:00\n",
      "   Estaciones incluidas: ['NOROESTE' 'NOROESTE2' 'NOROESTE3']\n",
      "   Columnas disponibles: 20\n",
      "\n",
      "RESUMEN DETALLADO:\n",
      "----------------------------------------\n",
      "Registros por estación:\n",
      "  NOROESTE:\n",
      "     Registros: 54446\n",
      "     Fechas: 2020-01-01 00:00:00 a 2024-12-31 23:00:00\n",
      "     Períodos: 2020-2021, 2022-2023, 2023-2024, 2024\n",
      "\n",
      "  NOROESTE2:\n",
      "     Registros: 54442\n",
      "     Fechas: 2020-01-01 00:00:00 a 2024-12-31 23:00:00\n",
      "     Períodos: 2020-2021, 2022-2023, 2023-2024, 2024\n",
      "\n",
      "  NOROESTE3:\n",
      "     Registros: 28889\n",
      "     Fechas: 2022-12-01 01:00:00 a 2024-12-31 23:00:00\n",
      "     Períodos: 2022-2023, 2023-2024, 2024\n",
      "\n",
      "ANÁLISIS DE COLUMNAS:\n",
      "------------------------------\n",
      "Parámetros disponibles: ['CO', 'NO', 'NO2', 'NOX', 'O3', 'PM10', 'PM25', 'PRS', 'RAINF', 'RH', 'SO2', 'SR', 'TOUT', 'WSR', 'WDR', 'WDV']\n",
      "\n",
      "CALIDAD DE DATOS:\n",
      "-------------------------\n",
      "  CO: 12.4% datos faltantes\n",
      "  NO: 10.4% datos faltantes\n",
      "  NO2: 13.5% datos faltantes\n",
      "  NOX: 11.5% datos faltantes\n",
      "  O3: 13.6% datos faltantes\n",
      "     Fechas: 2020-01-01 00:00:00 a 2024-12-31 23:00:00\n",
      "     Períodos: 2020-2021, 2022-2023, 2023-2024, 2024\n",
      "\n",
      "  NOROESTE2:\n",
      "     Registros: 54442\n",
      "     Fechas: 2020-01-01 00:00:00 a 2024-12-31 23:00:00\n",
      "     Períodos: 2020-2021, 2022-2023, 2023-2024, 2024\n",
      "\n",
      "  NOROESTE3:\n",
      "     Registros: 28889\n",
      "     Fechas: 2022-12-01 01:00:00 a 2024-12-31 23:00:00\n",
      "     Períodos: 2022-2023, 2023-2024, 2024\n",
      "\n",
      "ANÁLISIS DE COLUMNAS:\n",
      "------------------------------\n",
      "Parámetros disponibles: ['CO', 'NO', 'NO2', 'NOX', 'O3', 'PM10', 'PM25', 'PRS', 'RAINF', 'RH', 'SO2', 'SR', 'TOUT', 'WSR', 'WDR', 'WDV']\n",
      "\n",
      "CALIDAD DE DATOS:\n",
      "-------------------------\n",
      "  CO: 12.4% datos faltantes\n",
      "  NO: 10.4% datos faltantes\n",
      "  NO2: 13.5% datos faltantes\n",
      "  NOX: 11.5% datos faltantes\n",
      "  O3: 13.6% datos faltantes\n"
     ]
    }
   ],
   "source": [
    "# EJECUTAR EL PROCESO COMPLETO\n",
    "print(\"EJECUTANDO PROCESO DE CONCATENACIÓN COMPLETO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ejecutar proceso principal\n",
    "noroeste_dataset, raw_data = process_all_noroeste_data()\n",
    "\n",
    "# Mostrar resumen de resultados\n",
    "if noroeste_dataset is not None:\n",
    "    print(\"\\nRESUMEN DETALLADO:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Información por estación\n",
    "    station_summary = noroeste_dataset.groupby('station').agg({\n",
    "        'datetime': ['count', 'min', 'max'],\n",
    "        'year_range': 'unique'\n",
    "    })\n",
    "    \n",
    "    print(\"Registros por estación:\")\n",
    "    for station in noroeste_dataset['station'].unique():\n",
    "        station_data = noroeste_dataset[noroeste_dataset['station'] == station]\n",
    "        print(f\"  {station}:\")\n",
    "        print(f\"     Registros: {len(station_data)}\")\n",
    "        print(f\"     Fechas: {station_data['datetime'].min()} a {station_data['datetime'].max()}\")\n",
    "        print(f\"     Períodos: {', '.join(station_data['year_range'].unique())}\")\n",
    "        print()\n",
    "    \n",
    "    # Verificar columnas comunes\n",
    "    print(\"ANÁLISIS DE COLUMNAS:\")\n",
    "    print(\"-\" * 30)\n",
    "    parameter_columns = [col for col in noroeste_dataset.columns \n",
    "                        if col not in ['datetime', 'station', 'year_range', 'source_file']]\n",
    "    print(f\"Parámetros disponibles: {parameter_columns}\")\n",
    "    \n",
    "    # Verificar calidad de datos\n",
    "    print(\"\\nCALIDAD DE DATOS:\")\n",
    "    print(\"-\" * 25)\n",
    "    for param in parameter_columns[:5]:  # Mostrar primeros 5 parámetros\n",
    "        if param in noroeste_dataset.columns:\n",
    "            missing_pct = (noroeste_dataset[param].isna().sum() / len(noroeste_dataset)) * 100\n",
    "            print(f\"  {param}: {missing_pct:.1f}% datos faltantes\")\n",
    "else:\n",
    "    print(\"[ERROR] No se pudo generar el dataset final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c79436f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GUARDANDO DATOS PROCESADOS...\n",
      "[OK] Datos guardados en: NOROESTE_CONCATENATED_DATA.csv\n",
      "[OK] Resumen guardado en: NOROESTE_SUMMARY.csv\n",
      "\n",
      "PROCESO COMPLETADO EXITOSAMENTE!\n",
      "Dataset final: 137777 registros\n",
      "Estaciones: 3\n",
      "Rango temporal: 2020-01-01 00:00:00 a 2024-12-31 23:00:00\n",
      "\n",
      "PRÓXIMOS PASOS RECOMENDADOS:\n",
      "========================================\n",
      "1. Análisis exploratorio de datos (EDA)\n",
      "2. Limpieza adicional (outliers, valores inconsistentes)\n",
      "3. Visualización temporal por estación\n",
      "4. Análisis de correlaciones entre estaciones\n",
      "5. Identificación de patrones estacionales\n",
      "6. Detección de anomalías\n",
      "7. Preparación para análisis específicos\n",
      "\n",
      "============================================================\n",
      "CONCATENACIÓN DE DATOS NOROESTE FINALIZADA\n",
      "============================================================\n",
      "[OK] Datos guardados en: NOROESTE_CONCATENATED_DATA.csv\n",
      "[OK] Resumen guardado en: NOROESTE_SUMMARY.csv\n",
      "\n",
      "PROCESO COMPLETADO EXITOSAMENTE!\n",
      "Dataset final: 137777 registros\n",
      "Estaciones: 3\n",
      "Rango temporal: 2020-01-01 00:00:00 a 2024-12-31 23:00:00\n",
      "\n",
      "PRÓXIMOS PASOS RECOMENDADOS:\n",
      "========================================\n",
      "1. Análisis exploratorio de datos (EDA)\n",
      "2. Limpieza adicional (outliers, valores inconsistentes)\n",
      "3. Visualización temporal por estación\n",
      "4. Análisis de correlaciones entre estaciones\n",
      "5. Identificación de patrones estacionales\n",
      "6. Detección de anomalías\n",
      "7. Preparación para análisis específicos\n",
      "\n",
      "============================================================\n",
      "CONCATENACIÓN DE DATOS NOROESTE FINALIZADA\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# GUARDAR DATOS PROCESADOS\n",
    "if noroeste_dataset is not None:\n",
    "    print(\"GUARDANDO DATOS PROCESADOS...\")\n",
    "    \n",
    "    # Guardar como CSV\n",
    "    output_file = 'NOROESTE_CONCATENATED_DATA.csv'\n",
    "    noroeste_dataset.to_csv(output_file, index=False)\n",
    "    print(f\"[OK] Datos guardados en: {output_file}\")\n",
    "    \n",
    "    # Guardar resumen por estación\n",
    "    summary_file = 'NOROESTE_SUMMARY.csv'\n",
    "    station_summary = noroeste_dataset.groupby(['station', 'year_range']).agg({\n",
    "        'datetime': ['count', 'min', 'max']\n",
    "    }).reset_index()\n",
    "    station_summary.to_csv(summary_file, index=False)\n",
    "    print(f\"[OK] Resumen guardado en: {summary_file}\")\n",
    "    \n",
    "    print(f\"\\nPROCESO COMPLETADO EXITOSAMENTE!\")\n",
    "    print(f\"Dataset final: {len(noroeste_dataset)} registros\")\n",
    "    print(f\"Estaciones: {len(noroeste_dataset['station'].unique())}\")\n",
    "    print(f\"Rango temporal: {noroeste_dataset['datetime'].min()} a {noroeste_dataset['datetime'].max()}\")\n",
    "\n",
    "# PRÓXIMOS PASOS RECOMENDADOS\n",
    "print(\"\\nPRÓXIMOS PASOS RECOMENDADOS:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"1. Análisis exploratorio de datos (EDA)\")\n",
    "print(\"2. Limpieza adicional (outliers, valores inconsistentes)\")\n",
    "print(\"3. Visualización temporal por estación\")\n",
    "print(\"4. Análisis de correlaciones entre estaciones\")\n",
    "print(\"5. Identificación de patrones estacionales\")\n",
    "print(\"6. Detección de anomalías\")\n",
    "print(\"7. Preparación para análisis específicos\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONCATENACIÓN DE DATOS NOROESTE FINALIZADA\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
